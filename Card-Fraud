import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
#忽略警告信息
warnings.filterwarnings('ignore')
pd.set_option('display.max_row',None)
pd.set_option('display.max_columns',None)
#设置字体防止乱码
plt.rcParams['font.family'] = ['Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

df = pd.read_csv('/card_transdata.csv')#导入数据集
df.head()#显示前五行

df.info()

print('数据维度\n',df.shape)#显示数据维度    

print('缺失值\n',df.isnull().sum())#查找缺失值

print('重复值\n',df.duplicated().sum())#查找重复值

df.describe()#判断是否存在极值

n_sample=df.shape[0]
n_0_sample=df.fraud.value_counts()[0]
n_1_sample=df.fraud.value_counts()[1]
print('样本个数：{}；0占{}；1占{}'.format(n_sample,n_0_sample/n_sample,n_1_sample/n_sample))#显示数据集中0和1的分别占比

#数据分布详细：生成柱状图和饼图#
df_t=df.fraud.value_counts().reset_index()
df_t.replace({0:'NoFraud',1:'Fraud'},inplace=True)

fig=plt.figure(figsize=(10,4))
ax1=fig.add_subplot(1,2,1)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
sns.barplot(x="index",y="fraud",data=df_t)
plt.xlabel('')
plt.ylabel('count',fontdict={'fontsize':12})
ax2=fig.add_subplot(1,2,2)
plt.pie(x=df_t['fraud'],labels=df_t['index'],autopct='%1.1f%%',explode=[0.1,0],startangle=90,counterclock=False,wedgeprops={'linewidth':1,'edgecolor':"black"})
plt.axis('square')
plt.show()

#数据内部相关性热力图#
correlation_matrix = df.corr()
import seaborn as sns
sns.heatmap(correlation_matrix,annot=True,cmap='coolwarm',linewidths=1)

#对目标变量进行过采样#
X = df.iloc[:,:-1]#特征列
y = df.iloc[:,-1]#目标列

from imblearn.over_sampling import SMOTE
smote=SMOTE()
X_resampled,y_resampled = smote.fit_resample(X,y)

#查看过采样后的数据
n_sample_new = X_resampled.shape[0]
n_0_sample_new = y_resampled.value_counts()[0]
n_1_sample_new = y_resampled.value_counts()[1]
print('样本个数：{}；0占{}；1占{}'.format(n_sample_new,n_0_sample_new/n_sample_new,n_1_sample_new/n_sample_new))

#划分训练集和测试集
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_resampled,y_resampled,test_size=0.2,random_state=2023)

#逻辑回归
from sklearn.linear_model import LogisticRegression as LR
model_LR = LR(penalty="l2")
model_LR.fit(X_train,y_train)

model_LR.score(X_test,y_test)

#做五折交叉验证：将数据集划分为五个大小相同的份，循环5次，每次取4份做训练集，1份做验证集
#使用4份训练集对模型进行训练，并在验证集上进行评估，可计算模型的性能指标
#完成5次循环后，可以将五次循环中得到的性能指标取平均值，作为模型性能的最终评估结果
from sklearn.model_selection import cross_val_score as CVS
CVS(model_LR,X_train,y_train,cv = 5).mean()

#XGBoost
from xgboost import XGBClassifier as XGBC
model_xgb = XGBC(n_estimators=100,max_depth=3)
model_xgb.fit(X_train,y_train)

model_xgb.score(X_test,y_test)

CVS(model_xgb,X_train,y_train,cv = 5).mean()

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
y_test_pred = model_xgb.predict(X_test)
auc = accuracy_score(y_test,y_test_pred)
print('准确率为：{:.2f}%'.format(auc))

recall = recall_score(y_test, y_test_pred)
print('召回率为：{:.2f}%'.format(recall))

pre = precision_score(y_test, y_test_pred)
print('精确度率为：{:.2f}%'.format(pre))

f1 = f1_score(y_test, y_test_pred)
print('F1-Score为：{:.2f}%'.format(f1))

model_xgb.feature_importances_
